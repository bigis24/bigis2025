
<!-- saved from url=(0046)https://mm2022-apccpa-workshop.github.io/#call -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- SITE TITTLE -->
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Workshop BIGIS 2025</title>
    
    <!-- PLUGINS CSS STYLE -->
    <!-- link rel="icon" href="https://2022.acmmm.org/wp-content/themes/acmmultimedia/assets/images/favicon-acm.png" -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/jquery-ui.min.css" rel="stylesheet">
    <!-- Bootstrap -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/font-awesome.min.css" rel="stylesheet">
    <!-- Owl Carousel -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/slick.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/slick-theme.css" rel="stylesheet">
    <!-- Fancy Box -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/jquery.fancybox.pack.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/nice-select.css" rel="stylesheet">
    <link href="./LGM3A_Workshop_ACMMM2025_files/bootstrap-slider.min.css" rel="stylesheet">
    <!-- CUSTOM CSS -->
    <link href="./LGM3A_Workshop_ACMMM2025_files/style.css" rel="stylesheet">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
   <style type="text/css">
    .left,
    .right {
      float: left;
      width: 50%;
      padding-left: 80px;
    }
    .left { padding-left: 250px; }
  </style> 
    </head>
    
    <body class="body-wrapper" data-new-gr-c-s-check-loaded="14.997.0" data-gr-ext-installed="" style="background:hsl(108, 41%, 73%);">
        <section>
          <div class="container">
            <div class="row">
              <div class="col-md-12">
                <nav class="navbar navbar-expand-lg  navigation"> <a class="navbar-brand" href="https://www3.cs.stonybrook.edu/~icdm2025/index.html"> <img src="./LGM3A_Workshop_ACMMM2025_files\icdm2025logosmall.png" alt="" width="100px"> </a>
                  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
                  <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ml-auto main-nav">
                      <li class="nav-item"> <a class="nav-link" href="https://bigis24.github.io/bigis2025/#overview"><font color="white"><strong>Home</strong></font></a></li>
					  
					  <li class="nav-item"> <a class="nav-link " href="https://bigis24.github.io/bigis2025/#announcements"><font color="white"><strong>News</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link " href="https://bigis24.github.io/bigis2025/#call"><font color="white"><strong>Call for papers</strong></font></a></li>
      
					  <li class="nav-item"> <a class="nav-link" href="https://bigis24.github.io/bigis2025/#submission"><font color="white"><strong>Submission</strong></font></a></li>
	  
                      <li class="nav-item"> <a class="nav-link" href="https://bigis24.github.io/bigis2025/#organizers"><font color="white"><strong>Organizers</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link" href="https://bigis24.github.io/bigis2025/#speakers"><font color="white"><strong>Speakers</strong></font></a></li>
    
                      <li class="nav-item"> <a class="nav-link" href="https://bigis24.github.io/bigis2025/#schedule"><font color="white"><strong>Schedule</strong></font></a></li>
                    </ul>
                    
                  </div>
                </nav>
              </div>
            </div>
          </div>
        </section>
    
    
    <!--==========================================
    =            Overview Section            =
    ===========================================-->
    
    <section class="popular-deals section bg-white"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-md-12"> 
            <!-- Section title -->
            <div class="section-title">
              <h1>Information Seeking with Big Models (BigIS)</h1>
			  <br>
			  <h4>Workshop at IEEE ICDM 2025</h4>
			  <br><br>
              <h2 id="overivew">Scope and Topics</h2>
            </div>
        This workshop is dedicated to pioneering research at the intersection of information seeking (e.g., information retrieval, recommendation, and question-answering), with the transformative potential of big models (e.g., language, visual, audio, and multimodal models), specifically focusing on five key aspects. (1) It encourages exploring how big models can revolutionize information retrieval strategies, emphasizing advancements that enhance efficiency and effectiveness in accessing information. (2) We encourage active researchers to utilize big models to advance recommender algorithms, enhancing user modeling for improved personalized recommendations. (3) We encourage groundbreaking research in question-answering systems, leveraging the capabilities of big models to provide accurate and contextually relevant responses. (4) We emphasize the critical role of trustworthiness when employing big models for information seeking, to ensure the reliability of generated content with ethical and legal standards. (5) We encourage researchers to design robust evaluation methodologies, standards, and human evaluation paradigms to comprehensively assess the impact of big models in information seeking. 
        </div>
        </div>
      </div>
    </section>
    

    
    <!--===========================================
    =            Dates Section            =
    ============================================-->

    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="announcements">News</h2>
            </div>
			<ul style="margin-left: 40px">
      <!-- <li>28/8/2023 - Workshop schedule is announced. </li>
      <li>8/8/2023 - Workshop papers notification is announced. </li>
      <li>19/7/2023 - Workshop papers submission is delayed. </li>
      <li>27/4/2023 - Important dates are updated. </li> -->
      <li>16 April 2025 - Call For Paper is released. </li> 
	  <li>16 April 2025 - Workshop homepage is now available. </li>
			</ul>

          </div>
        </div>
      </div>
    </section>

	
	<!--==========================================
    =            Call for Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="call">Call for Papers</h2>
            </div>
            The workshop centers around pioneering research at the intersection of information seeking, big models, and novel technologies. It specifically focuses on five key aspects: (1) revolutionizing information retrieval, e.g., exploring how big models can enhance efficiency and effectiveness in accessing information; (2) advancing recommender algorithms, e.g., utilizing big models to improve personalized recommendations; (3) enhancing question-answering systems, e.g., leveraging big models for accurate and contextually relevant responses; (4) ensuring trustworthiness, e.g., emphasizing ethical and legal standards when using big models; (5) comprehensive evaluation, e.g., designing robust evaluation methodologies to assess big models' impact.
            We welcome original submissions across a wide range of topics, including but not limited to:
			<ul style="margin-left: 40px">

      <li>Information retrieval with large generative models</li>
      <li>Query expansion and reformulation strategies</li>
      <li>User studies and experiences with big models</li>
      <li>Ethical considerations and responsible AI</li>
      <li>Architectural integration into search engines</li>
      <li>Scalability challenges in deploying big models </li>
      <li>Applications in specialized domains</li>
      <li>Robust evaluation metrics</li>
      <li>Multimodal information seeking</li>
      <li>Integration in recommendation systems</li>
      <li>Advancements in question-answering</li>
			</ul>

      Important dates: 
			<ul style="margin-left: 40px">

        <li>Workshop Papers Submission: <strong>7 September 2025</strong></li>
	    <li>Workshop Papers Notification: <strong>15 September 2025</strong></li>
        <li>Camera-ready Submission: <strong>25 September 2025</strong></li>
		<li>Workshop date: <strong>12 November 2025</strong></li>
        </ul>
		Please note: The submission deadline is at 11:59 p.m. of the stated deadline date&nbsp;<a href="https://time.is/Anywhere_on_Earth"><font color="red"><u>Anywhere on Earth</u></font></a>.</p>

          </div>
        </div>
      </div>
    </section>

    <!--==========================================
    =            Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="submission">Submission</h2>
            </div>
            <li><strong>Submission Guidelines</strong>:</li>
			Submitted papers (.pdf format) must be the same format & template as the main conference. 
      Please remember to add Concepts and Keywords. All the accepted papers can be up to 10 pages including references. All papers will undergo the same review process and review period. Paper submissions must conform to the ``double-blind'' review policy. All papers will be peer-reviewed by experts in the field. Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality.
      <!--The workshop papers will be published in the ACM Digital Library.--> 
			<li><strong>Submission Site</strong>: <a href="https://wi-lab.com/cyberchair/2025/icdm25/scripts/submit.php?subarea=S13&undisplay_detail=1&wh=/cyberchair/2025/icdm25/scripts/ws_submit.php"><font color=#5672f9><u>Submission Link</u></font></a> 
			<!-- <a href="https://easychair.org/conferences/?conf=lgm3a"><font color=#5672f9><u>https://easychair.org/conferences/?conf=lgm3a</u></font></a></li> -->
          </div>
        </div>
      </div>
    </section>	



	
    <!--==========================================
    =            Organizers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="organizers">Organizers</h2>
            </div>
			<ul style="margin-left: 40px">
			<!-- <li><a href="https://zhengwang125.github.io/"><font color=#5672f9><u>Zheng Wang</u></font></a> (Huawei Singapore Research Center, Singapore)</li>
			<li><a href="https://personal.ntu.edu.sg/c.long/index.html"><font color=#5672f9><u>Cheng Long</u></font></a> (Nanyang Technological University, Singapore)</li>
			<li><font color=#5672f9><u>Shihao Xu</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Bingzheng Gan</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Wei Shi</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><a href="https://scholar.google.com/citations?user=aJmTPaoAAAAJ&hl=en"><font color=#5672f9><u>Zhao Cao</u></font></a> (Huawei Technologies Co., Ltd, China)</li>
			<li><a href="https://www.chuatatseng.com/"><font color=#5672f9><u>Tat-Seng Chua</u></font></a> (National University of Singapore, Singapore)</li> -->

    
	<li><a href="https://zhengwang125.github.io/"><font color=#5672f9><u>Zheng Wang</u></font></a> (Huawei Singapore Research Center, Singapore)</li>
	<li><font color=#5672f9><u>Jieer Ouyang</u></font> (Huawei Singapore Research Center, Singapore)</li>
    <li><a href="https://www.linkedin.com/in/xiaoneng-hsiang/"><font color=#5672f9><u>Xiaoneng Xiang</u></font></a> (Huawei Singapore Research Center, Singapore)</li>
    <li><font color=#5672f9><u>Wei Shi</u></font> (Huawei Singapore Research Center, Singapore)</li>
    <li><a href="https://sora-city.github.io/"><font color=#5672f9><u>Kun Tan</u></font></a> (Huawei Technologies, Co., Ltd.)</li>
	<li><a href="https://hexiangnan.github.io/"><font color=#5672f9><u>Xiangnan He</u></font></a> (University of Science and Technology of China, China)</li>
  <li><a href="https://personal.ntu.edu.sg/c.long/"><font color=#5672f9><u>Cheng Long</u></font></a> (Nanyang Technological University, Singapore)</li>
    <li><a href="https://personal.ntu.edu.sg/gaocong/"><font color=#5672f9><u>Gao Cong</u></font></a> (Nanyang Technological University, Singapore)</li>
			</ul>

    <!--==========================================
    =            Speakers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="speakers">Speakers</h2>
            </div>
        
		
	        <h3><b>Keynote 1</b></h3> 
                    
            TBD
            <!-- <img src="https://liuziwei7.github.io/homepage_files/me.png" class="media-left pull-left" width="170" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="https://liuziwei7.github.io/"><font color=#5672f9><u>Prof. Ziwei Liu</u></font></a> is a Nanyang Assistant Professor (2020-) at College of Computing and Data Science in Nanyang Technological University, with MMLab@NTU. Previously, he was a research fellow (2018-2020) in CUHK with Prof. Dahua Lin and a post-doc researcher (2017-2018) in UC Berkeley with Prof. Stella Yu. His research interests include computer vision, machine learning and computer graphics. Ziwei received his Ph.D. (2013-2017) from CUHK, Multimedia Lab, advised by Prof. Xiaoou Tang and Prof. Xiaogang Wang. He is fortunate to have internships at Microsoft Research and Google Research. Ziwei is the recipient of MIT Technology Review Innovators under 35 Asia Pacific, ICBS Frontiers of Science Award, CVPR Best Paper Award Candidate and WAIC Yunfan Award. His works have been transferred to products, including Microsoft Pix, SenseGo and Google Clips.
            </p>

            <b>Talk Title:</b> Multi-Modal Generative AI with Foundation Models
            </br>
            <b>Abstract:</b> Generating photorealistic and controllable visual contents has been a long-pursuing goal of artificial intelligence (AI), with extensive real-world applications. It is also at the core of embodied intelligence. In this talk, I will discuss our work in AI-driven visual context generation of humans, objects and scenes, with an emphasis on combining the power of neural rendering with large multimodal foundation models. Our generative AI framework has shown its effectiveness and generalizability on a wide range of tasks. -->

	        <br></br> 
			
            <h3><b>Keynote 2</b></h3> 
                    
            TBD
            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->
            
            <!-- <img src="https://cde.nus.edu.sg/ece/wp-content/uploads/sites/3/2021/05/zhengshou_stanford_photo.jpg" class="media-left pull-left" width="170" height="180" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="https://sites.google.com/view/showlab"><font color=#5672f9><u>Prof. Mike Zheng Shou</u></font></a> is a tenure-track Assistant Professor at National University of Singapore and a former Research Scientist at Facebook AI in the Bay Area. He holds a PhD degree from Columbia University in the City of New York, where he worked with Prof. Shih-Fu Chang. He was awarded the Wei Family Private Foundation Fellowship. He received the best paper finalist at CVPR'22 and the best student paper nomination at CVPR'17. His team won 1st place in multiple international challenges including ActivityNet 2017, EPIC-Kitchens 2022, Ego4D 2022 & 2023. He is a Fellow of the National Research Foundation (NRF) Singapore and has been named on the Forbes 30 Under 30 Asia list. 
            
            </p>

            <b>Talk Title:</b> Multimodal Video Understanding and Generation
            </br>
            <b>Abstract:</b> Exciting progress has been made in multimodal video intelligence, including both understanding and generation, these two pillars in video. Despite being promising, several key challenges still remain. In this talk, I will introduce our attempts to address some of them. (1) For understanding, I will share All-in-one, which employs one single unified network for efficient video-language modeling, and EgoVLP, which is the first video-language pre-trained model for egocentric video. (2) For generation, I will introduce our study of efficient video diffusion models (i.e., Tune-A-Video, 4K GitHub stars). (3) Finally, I would like to discuss our recent exploration, Show-o, one single LLM that unifies multimodal understanding and generation. -->
            
			<br></br> 

            <h3><b>Keynote 3</b></h3> 
                    
            TBD
            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->
            
            <!-- <img src="https://cde.nus.edu.sg/ece/wp-content/uploads/sites/3/2021/05/zhengshou_stanford_photo.jpg" class="media-left pull-left" width="170" height="180" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="https://sites.google.com/view/showlab"><font color=#5672f9><u>Prof. Mike Zheng Shou</u></font></a> is a tenure-track Assistant Professor at National University of Singapore and a former Research Scientist at Facebook AI in the Bay Area. He holds a PhD degree from Columbia University in the City of New York, where he worked with Prof. Shih-Fu Chang. He was awarded the Wei Family Private Foundation Fellowship. He received the best paper finalist at CVPR'22 and the best student paper nomination at CVPR'17. His team won 1st place in multiple international challenges including ActivityNet 2017, EPIC-Kitchens 2022, Ego4D 2022 & 2023. He is a Fellow of the National Research Foundation (NRF) Singapore and has been named on the Forbes 30 Under 30 Asia list. 
            
            </p>

            <b>Talk Title:</b> Multimodal Video Understanding and Generation
            </br>
            <b>Abstract:</b> Exciting progress has been made in multimodal video intelligence, including both understanding and generation, these two pillars in video. Despite being promising, several key challenges still remain. In this talk, I will introduce our attempts to address some of them. (1) For understanding, I will share All-in-one, which employs one single unified network for efficient video-language modeling, and EgoVLP, which is the first video-language pre-trained model for egocentric video. (2) For generation, I will introduce our study of efficient video diffusion models (i.e., Tune-A-Video, 4K GitHub stars). (3) Finally, I would like to discuss our recent exploration, Show-o, one single LLM that unifies multimodal understanding and generation. -->
         <br></br> 
		 
    <!--===========================================
    =            Workshop Arrangement            =
    ============================================-->
    
    <section class="popular-deals section bg-white">
      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <div class="section-title">
              <h2 id="schedule">Workshop Schedule</h2>
            </div>
            <p align="center">
                <!-- <img src="./res/schedule.png" width="90%"> -->
                <!-- <big><b>TBD</b></big> -->
                <!-- <font color="Blue"><i><strong>TBA </strong></i></font> -->
                <li>On-site venue: TBD</li>
                <li>Date & Time: TBD</li>
                <!-- <li>Zoom link: <a href=""><font color=#5672f9><u>TBD</u></font></a></li> -->
                <li>Zoom link: <a><font color=#5672f9><u>TBD</u></font></a></li>
                  <br/>
                  <style type="text/css">
                    #myta td{
                      padding-right: 8px;
                    }
                  </style>
                  <table border="1" id="myta">

                    <tr> <td><strong>Time (GMT+11) </strong></td> <td>	<strong>Title</strong>	</td></tr>
                    <!-- <tr> <td>13:00 - 13:10</td> <td>	Welcome Message from the Chairs	</td></tr>
                    <tr> <td>13:10 - 13:50</td> <td>	Keynote 1: Multi-Modal Generative AI with Foundation Models </td></tr>
                    <tr> <td>13:50 - 14:30</td> <td>	Keynote 2: Multimodal Video Understanding and Generation </td></tr>
                    <tr> <td>14:30 - 14:40</td> <td>	Break </td></tr>
                    <tr> <td>14:40 - 15:00</td> <td>	Presentation 1: Geo-LLaVA: A Large Multi-Modal Model for Solving Geometry Math Problems with Meta In-Context Learning </td></tr>
                    <tr> <td>15:00 - 15:20</td> <td>	Presentation 2: Leveraging the Syntactic Structure of the Text Prompt to Enhance Object-Attribute Binding in Image Generation </td></tr>
                    <tr> <td>15:20 - 15:40</td> <td>	Presentation 3: SynthDoc: Bilingual Documents Synthesis for Visual Document Understanding </td></tr>
                    <tr> <td>15:40 - 16:00</td> <td>	Presentation 4: Multimodal Understanding: Investigating the Capabilities of Large Multimodel Models for Object Detection in XR Applications </td></tr>
                    <tr> <td>16:00 - 16:20</td> <td>	Presentation 5: A Method for Efficient Structured Data Generation with Large Language Models </td></tr>
                    <tr> <td>16:20 - 16:30</td> <td>	Workshop Closing </td></tr> -->
                    </table>
<br>
<br>
                    <!--
                    <table border="1">
                    <tbody><tr> <td><strong>Time (October 14, Local Portugal Time)</strong></td> <td>	<strong>Paper Title</strong>	</td></tr>
                    <tr> <td>14:00 PM - 14:15 PM</td> <td>	IPDAE: Improved Patch-Based Deep Autoencoder for Lossy Point Cloud Geometry Compression	</td></tr>
                    <tr> <td>14:15 PM - 14:30 PM</td> <td>	GRASP-Net: Geometric Residual Analysis and Synthesis for Point Cloud Compression	</td></tr>
                    <tr> <td>14:30 PM - 14:45 PM</td> <td>	Wiener Filter-Based Point Cloud Adaptive Denoising for Video-based Point Cloud Compression	</td></tr>
                    <tr> <td>14:45 PM - 15:00 PM</td> <td>	OpenPointCloud-V2: A Deep Learning Based Open-Source Algorithm Library of Point Cloud Processing	</td></tr>

                    <tr> <td>15:00 PM - 15:15 PM</td> <td>	End-to-End Point Cloud Geometry Compression and Analysis with Sparse Tensor	</td></tr>
                    <tr> <td>15:15 PM - 15:30 PM</td> <td>	Transformer and Upsampling-Based Point Cloud Compression	</td></tr>
                    <tr> <td>15:30 PM - 15:45 PM</td> <td>	Quality Evaluation of Machine Learning-based Point Cloud Coding Solutions	</td></tr>
                    <tr> <td>15:45 PM - 16:00 PM</td> <td>	View-Adaptive Streaming of Point Cloud Scenes through Combined Decomposition and Video-based Coding	</td></tr>
                    </tbody></table> -->
				
            </p>
          </div>
        </div>
      </div>
    </section>


    
    <!--============================
    =            Footer            =
    =============================-->
    
    <!-- Footer Bottom -->
    <!-- footer class="footer-bottom"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-sm-12 col-12"> 
            <!-- Copyright -->
            <div class="copyright">
              <p class="text-center">
                For any questions, please email to <a href="mailto:zheng011@e.ntu.edu.sg"><font color=#5672f9><u>zheng011@e.ntu.edu.sg</u></a>
              </p>
                <br class="clear">
            </div>
          </div>
        </div>
      </div>
      <!-- Container End --> 
    </footer>
    
    <!-- JAVASCRIPTS --> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery-ui.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/tether.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.raty-fa.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/popper.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/bootstrap.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/bootstrap-slider.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/slick.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.nice-select.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/jquery.fancybox.pack.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/SmoothScroll.min.js.download"></script> 
    <script src="./LGM3A_Workshop_ACMMM2025_files/scripts.js.download"></script -->
    
    
    

</body></html>
